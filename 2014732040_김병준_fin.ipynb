{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2014732040_김병준_fin.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOoD4YjbL9M1yjTgsMlhfHh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZVh_SSx0yha","executionInfo":{"status":"ok","timestamp":1639637841948,"user_tz":-540,"elapsed":3528,"user":{"displayName":"Hey Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAJdM5xi7Jco580i60XCozfsYSSrkt5l2-tcrduA=s64","userId":"08619336077776558055"}},"outputId":"ac3c7cc4-eac2-4663-fd97-0eb38636bf04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","cuda\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchtext.legacy import data, datasets \n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive')\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","\n","# for reproducibility\n","torch.manual_seed(777)\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(777)\n","\n","# parameters\n","BATCH_SIZE = 64\n","learning_rate = 0.001\n","training_epochs = 10"]},{"cell_type":"code","source":["TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n","LABEL = data.Field(sequential=False, batch_first=True)\n","trainset, valset, testset = datasets.SST.splits(TEXT, LABEL)\n","\n","# data.Field.build_vocab() 라이브러리\n","# 문장 내 단어와 Integer index 를 매칭시키는 단어장(vocab)을 생성 == 워드 임베딩을 위한 Vocab 생성\n","# <UNK> = 0, <PAD> = 1 토큰도 추가.\n","# min_freq : 최소 5번 이상 등장한 단어들만 사전에 담겠다는 것. \n","# 5번 미만으로 등장하는 단어는 UNK라는 토큰으로 대체\n","\n","TEXT.build_vocab(trainset, min_freq=5)# TEXT 데이터를 기반으로 Vocab 생성\n","LABEL.build_vocab(trainset)# LABEL 데이터를 기반으로 Vocab 생성\n","\n","# 매 배치마다 비슷한 길이에 맞춰 줄 수 있도록 iterator 정의\n","train_iter, val_iter, test_iter = data.BucketIterator.splits(\n","        (trainset, valset, testset), batch_size=BATCH_SIZE,\n","        shuffle=True, repeat=False)\n","\n","vocab_size = len(TEXT.vocab)\n","n_classes = 3\n","\n","print(\"[TrainSet]: %d [ValSet]: %d [TestSet]: %d [Vocab]: %d [Classes] %d\"\n","      % (len(trainset),len(valset), len(testset), vocab_size, n_classes))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z99ovYEG1AEs","executionInfo":{"status":"ok","timestamp":1639637846545,"user_tz":-540,"elapsed":4607,"user":{"displayName":"Hey Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAJdM5xi7Jco580i60XCozfsYSSrkt5l2-tcrduA=s64","userId":"08619336077776558055"}},"outputId":"e7b4e6af-578c-4074-fd0b-b15f41c40b53"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[TrainSet]: 8544 [ValSet]: 1101 [TestSet]: 2210 [Vocab]: 3428 [Classes] 3\n"]}]},{"cell_type":"code","source":["class GRU(nn.Module):\n","    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n","        super(GRU, self).__init__()\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","\n","        self.embed = nn.Embedding(n_vocab, embed_dim)\n","        self.dropout = nn.Dropout(dropout_p)\n","        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n","                          num_layers=self.n_layers,\n","                          batch_first=True)\n","        self.out = nn.Linear(self.hidden_dim, n_classes)\n","\n","    def forward(self, x):\n","        x = self.embed(x)\n","        h_0 = self._init_state(batch_size=x.size(0)) # 첫번째 히든 스테이트를 0벡터로 초기화\n","        x, _ = self.gru(x, h_0)  # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)\n","        h_t = x[:,-1,:] # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.\n","        self.dropout(h_t)\n","        logit = self.out(h_t)  # (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)\n","        return logit\n","\n","    def _init_state(self, batch_size=1):\n","        weight = next(self.parameters()).data\n","        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"],"metadata":{"id":"kb3YraPc3FwI","executionInfo":{"status":"ok","timestamp":1639637846547,"user_tz":-540,"elapsed":21,"user":{"displayName":"Hey Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAJdM5xi7Jco580i60XCozfsYSSrkt5l2-tcrduA=s64","userId":"08619336077776558055"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["model = GRU(1, 256, vocab_size, 128, n_classes, 0.5).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"hBak8jXY3CEv","executionInfo":{"status":"ok","timestamp":1639637851380,"user_tz":-540,"elapsed":4850,"user":{"displayName":"Hey Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAJdM5xi7Jco580i60XCozfsYSSrkt5l2-tcrduA=s64","userId":"08619336077776558055"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 모델 훈련\n","def train(model, optimizer, train_iter):\n","    model.train()\n","    for b, batch in enumerate(train_iter):\n","        x, y = batch.text.to(device), batch.label.to(device)\n","        y.data.sub_(1)  # 레이블 값을 0과 1로 변환\n","        optimizer.zero_grad()\n","\n","        logit = model(x)\n","        loss = F.cross_entropy(logit, y)\n","        loss.backward()\n","        optimizer.step()"],"metadata":{"id":"kPJl2Bw_3U-k","executionInfo":{"status":"ok","timestamp":1639637851382,"user_tz":-540,"elapsed":19,"user":{"displayName":"Hey Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAJdM5xi7Jco580i60XCozfsYSSrkt5l2-tcrduA=s64","userId":"08619336077776558055"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# 모델 평가 함수\n","def evaluate(model, val_iter):\n","    \"\"\"evaluate model\"\"\"\n","    model.eval()\n","    corrects, total_loss = 0, 0\n","    for batch in val_iter:\n","        x, y = batch.text.to(device), batch.label.to(device)\n","        y.data.sub_(1) # 레이블 값을 0과 1로 변환\n","        logit = model(x)\n","        loss = F.cross_entropy(logit, y, reduction='sum')\n","        total_loss += loss.item()\n","        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n","    size = len(val_iter.dataset)\n","    avg_loss = total_loss / size\n","    avg_accuracy = 100.0 * corrects / size\n","    return avg_loss, avg_accuracy"],"metadata":{"id":"q0DKWjC63lGg","executionInfo":{"status":"ok","timestamp":1639637851383,"user_tz":-540,"elapsed":18,"user":{"displayName":"Hey Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAJdM5xi7Jco580i60XCozfsYSSrkt5l2-tcrduA=s64","userId":"08619336077776558055"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 16\n","learning_rate = 0.001\n","best_of_best_acc = None # 최종 최고 accuracy model\n","for i in range(0, 4): # batch size  2의 배수로 128 까지 증가\n","    while(True):\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","        train_iter, val_iter, test_iter = data.BucketIterator.splits(\n","                (trainset, valset, testset), batch_size=BATCH_SIZE,\n","                shuffle=True, repeat=False)\n","\n","        # 모델 훈련\n","        best_val_accuracy = None\n","        counting = 0\n","        print('모델 훈련시작!')\n","        print(f'현재 모델 Batch Size: {BATCH_SIZE} Learning Rate: {learning_rate}')\n","        for e in range(1, training_epochs+1):\n","            train(model, optimizer, train_iter)\n","            val_loss, val_accuracy = evaluate(model, val_iter)\n","        \n","            print(\"[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f\" % (e, val_loss, val_accuracy))\n","        \n","            # 검증 오차가 가장 적은 최적의 모델을 저장\n","            if not best_val_accuracy or val_accuracy > best_val_accuracy:\n","                if not os.path.isdir(\"snapshot\"):\n","                    os.makedirs(\"snapshot\")\n","                torch.save(model.state_dict(), './snapshot/model1.pt')\n","                best_val_accuracy = val_accuracy\n","                counting = 0 # accuracy 가 증가하면 다시 counting 0으로 초기화\n","            else:\n","                counting += 1\n","                if counting == 2:\n","                    print('Accuracy 가 2 epoch 동안 증가하지 않았으므로 훈련은 멈춥니다.')\n","                    break\n","        print(f'현재 모델 훈련 끝 Accuracy: {best_val_accuracy}')\n","        print('----------------------------------------------------------------\\n')\n","\n","        # 검증 오차가 가장 적은 최적의 모델을 저장\n","        if not best_of_best_acc or best_val_accuracy > best_of_best_acc:\n","            torch.save(model.state_dict(), './snapshot/model2.pt') # 최종 모델은 model2에 저장\n","            best_of_best_acc = best_val_accuracy\n","            print('최종 모델의 Accuracy가 증가하여 model을 업데이트 했습니다.')\n","        \n","        # learning rate가 0.1 이면 빠져나가기\n","        if learning_rate >= 0.1:\n","            break\n","        learning_rate += 0.002\n","    BATCH_SIZE *= 2 # batch size 2배씩 증가\n","    learning_rate = 0.001 # learning rate 0.001 부터 시작하기 위해 초기화\n","print(f'최종 모델의 Accuracy: {best_of_best_acc}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XWPogsRC6867","executionInfo":{"status":"ok","timestamp":1639638105936,"user_tz":-540,"elapsed":254569,"user":{"displayName":"Hey Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAJdM5xi7Jco580i60XCozfsYSSrkt5l2-tcrduA=s64","userId":"08619336077776558055"}},"outputId":"8332180a-ed5c-479e-a209-8f7286549b12"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["다음 모델 훈련시작!\n","현재 모델 Batch Size: 16 Learning Rate: 0.001\n","[Epoch: 1] val loss :  0.99 | val accuracy : 55.50\n","[Epoch: 2] val loss :  0.89 | val accuracy : 62.22\n","[Epoch: 3] val loss :  0.91 | val accuracy : 62.03\n","[Epoch: 4] val loss :  0.95 | val accuracy : 60.94\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 16 Learning Rate: 0.003\n","[Epoch: 1] val loss :  1.10 | val accuracy : 59.13\n","[Epoch: 2] val loss :  1.31 | val accuracy : 58.22\n","[Epoch: 3] val loss :  1.53 | val accuracy : 58.49\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 16 Learning Rate: 0.005\n","[Epoch: 1] val loss :  1.43 | val accuracy : 56.68\n","[Epoch: 2] val loss :  1.43 | val accuracy : 57.13\n","[Epoch: 3] val loss :  1.28 | val accuracy : 59.31\n","[Epoch: 4] val loss :  1.20 | val accuracy : 55.04\n","[Epoch: 5] val loss :  1.14 | val accuracy : 48.59\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 16 Learning Rate: 0.007\n","[Epoch: 1] val loss :  1.19 | val accuracy : 47.23\n","[Epoch: 2] val loss :  1.16 | val accuracy : 49.32\n","[Epoch: 3] val loss :  1.22 | val accuracy : 49.41\n","[Epoch: 4] val loss :  1.19 | val accuracy : 46.87\n","[Epoch: 5] val loss :  1.16 | val accuracy : 46.87\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 16 Learning Rate: 0.009000000000000001\n","[Epoch: 1] val loss :  1.18 | val accuracy : 46.96\n","[Epoch: 2] val loss :  1.14 | val accuracy : 50.32\n","[Epoch: 3] val loss :  1.31 | val accuracy : 45.41\n","[Epoch: 4] val loss :  1.20 | val accuracy : 47.41\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 16 Learning Rate: 0.011000000000000001\n","[Epoch: 1] val loss :  1.22 | val accuracy : 44.60\n","[Epoch: 2] val loss :  1.18 | val accuracy : 48.50\n","[Epoch: 3] val loss :  1.38 | val accuracy : 44.69\n","[Epoch: 4] val loss :  1.32 | val accuracy : 44.32\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 32 Learning Rate: 0.001\n","[Epoch: 1] val loss :  1.09 | val accuracy : 46.32\n","[Epoch: 2] val loss :  1.06 | val accuracy : 49.14\n","[Epoch: 3] val loss :  1.08 | val accuracy : 46.23\n","[Epoch: 4] val loss :  1.08 | val accuracy : 48.14\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 32 Learning Rate: 0.003\n","[Epoch: 1] val loss :  1.08 | val accuracy : 45.69\n","[Epoch: 2] val loss :  1.13 | val accuracy : 43.14\n","[Epoch: 3] val loss :  1.07 | val accuracy : 44.96\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 32 Learning Rate: 0.005\n","[Epoch: 1] val loss :  1.09 | val accuracy : 47.14\n","[Epoch: 2] val loss :  1.15 | val accuracy : 41.51\n","[Epoch: 3] val loss :  1.07 | val accuracy : 47.68\n","[Epoch: 4] val loss :  1.12 | val accuracy : 48.23\n","[Epoch: 5] val loss :  1.09 | val accuracy : 47.77\n","[Epoch: 6] val loss :  1.10 | val accuracy : 46.05\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 32 Learning Rate: 0.007\n","[Epoch: 1] val loss :  1.13 | val accuracy : 44.50\n","[Epoch: 2] val loss :  1.17 | val accuracy : 43.42\n","[Epoch: 3] val loss :  1.10 | val accuracy : 44.69\n","[Epoch: 4] val loss :  1.20 | val accuracy : 45.78\n","[Epoch: 5] val loss :  1.13 | val accuracy : 45.23\n","[Epoch: 6] val loss :  1.12 | val accuracy : 45.59\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 32 Learning Rate: 0.009000000000000001\n","[Epoch: 1] val loss :  1.17 | val accuracy : 46.59\n","[Epoch: 2] val loss :  1.13 | val accuracy : 45.96\n","[Epoch: 3] val loss :  1.08 | val accuracy : 46.78\n","[Epoch: 4] val loss :  1.12 | val accuracy : 46.59\n","[Epoch: 5] val loss :  1.12 | val accuracy : 44.41\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 32 Learning Rate: 0.011000000000000001\n","[Epoch: 1] val loss :  1.23 | val accuracy : 40.51\n","[Epoch: 2] val loss :  1.20 | val accuracy : 44.87\n","[Epoch: 3] val loss :  1.13 | val accuracy : 45.59\n","[Epoch: 4] val loss :  1.13 | val accuracy : 43.42\n","[Epoch: 5] val loss :  1.17 | val accuracy : 45.87\n","[Epoch: 6] val loss :  1.14 | val accuracy : 44.32\n","[Epoch: 7] val loss :  1.23 | val accuracy : 42.23\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 64 Learning Rate: 0.001\n","[Epoch: 1] val loss :  1.08 | val accuracy : 48.32\n","[Epoch: 2] val loss :  1.06 | val accuracy : 49.41\n","[Epoch: 3] val loss :  1.05 | val accuracy : 48.14\n","[Epoch: 4] val loss :  1.06 | val accuracy : 47.23\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 64 Learning Rate: 0.003\n","[Epoch: 1] val loss :  1.06 | val accuracy : 46.23\n","[Epoch: 2] val loss :  1.06 | val accuracy : 44.32\n","[Epoch: 3] val loss :  1.05 | val accuracy : 45.87\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 64 Learning Rate: 0.005\n","[Epoch: 1] val loss :  1.12 | val accuracy : 43.14\n","[Epoch: 2] val loss :  1.07 | val accuracy : 46.50\n","[Epoch: 3] val loss :  1.08 | val accuracy : 44.14\n","[Epoch: 4] val loss :  1.08 | val accuracy : 44.60\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 64 Learning Rate: 0.007\n","[Epoch: 1] val loss :  1.08 | val accuracy : 46.50\n","[Epoch: 2] val loss :  1.08 | val accuracy : 48.05\n","[Epoch: 3] val loss :  1.10 | val accuracy : 43.23\n","[Epoch: 4] val loss :  1.07 | val accuracy : 45.96\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 64 Learning Rate: 0.009000000000000001\n","[Epoch: 1] val loss :  1.10 | val accuracy : 44.50\n","[Epoch: 2] val loss :  1.15 | val accuracy : 46.96\n","[Epoch: 3] val loss :  1.08 | val accuracy : 46.05\n","[Epoch: 4] val loss :  1.10 | val accuracy : 44.69\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n","다음 모델 훈련시작!\n","현재 모델 Batch Size: 64 Learning Rate: 0.011000000000000001\n","[Epoch: 1] val loss :  1.10 | val accuracy : 45.14\n","[Epoch: 2] val loss :  1.18 | val accuracy : 43.69\n","[Epoch: 3] val loss :  1.14 | val accuracy : 43.42\n","현재 모델 훈련 끝 Accuracy: {best_val_accuracy}\n","----------------------------------------------------------------\n","\n"]}]},{"cell_type":"code","source":["model.load_state_dict(torch.load('./snapshot/model2.pt'))\n","test_loss, test_acc = evaluate(model, test_iter)\n","print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ACOdR4M60bX","executionInfo":{"status":"ok","timestamp":1639638105938,"user_tz":-540,"elapsed":46,"user":{"displayName":"Hey Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAJdM5xi7Jco580i60XCozfsYSSrkt5l2-tcrduA=s64","userId":"08619336077776558055"}},"outputId":"aea9addd-9f62-4fd9-e255-aea0e3baa16e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["테스트 오차:  0.88 | 테스트 정확도: 64.30\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"mPTKhAtk7_YV","executionInfo":{"status":"ok","timestamp":1639638105942,"user_tz":-540,"elapsed":35,"user":{"displayName":"Hey Jun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAJdM5xi7Jco580i60XCozfsYSSrkt5l2-tcrduA=s64","userId":"08619336077776558055"}}},"execution_count":9,"outputs":[]}]}